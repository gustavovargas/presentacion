<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<!-- , maximum-scale=1.0, user-scalable=no-->
		<title>TFM AFI</title>
		<meta name="description" content="Interpretabilidad, Machine Learning y Riesgo de Crédito">
		<meta name="author" content="Gustavo Vargas">
		<meta name="image" content="https://res.cloudinary.com/sagacity/image/upload/c_crop,h_1001,w_1500,x_0,y_0/c_limit,dpr_auto,f_auto,fl_lossy,q_80,w_1080/Kitten_murder_Jeff_Merkley_2_copy_hdpoxd.jpg">


		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
		
		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<div id="myLogo" style="background: url(lib/afi.png);
					position: fixed;
    					display: block;
					<!--top: 0%;-->
					<!--left: 90%;-->
					margin-top:10%;
					margin-right:0%;
					width: 20%;
					height: 25%;">
                        	</div>
				<section>
					<p>
						<h4>Trabajo Fin de Máster</h4>
					</p>
					<h2>
						Interpretabilidad, Machine Learning y Riesgo de Crédito
					</h2>

					<h4>
						Gustavo Vargas
					</h4>

					<p>
						<small>
							<a href="https://www.afiescueladefinanzas.es/master-big-data-finanzas">
								Máster en Big Data y Data Science en Finanzas</a>
						</small>
					</p>
				</section>

				<section>
					<section>
						<h2>
							Riesgo de crédito
						</h2>
						<ul>
							<li class="fragment grow">Probabilidad de incumplimiento</li>
							<li class="fragment grow">EL = PD × LGD × EAD</li>
						</ul>

					</section>

					<section>
						<h2>
							Probabilidad de incumplimiento
						</h2>

						<ul>
							<li>
							Riesgo de pérdida financiera producida por el incumplimiento o deterioro de la calidad crediticia de un cliente, al cual una entidad financiera ha financiado
							</li>
							<li>Credit Rating, Credit Scoring</li>
		
						</ul>
					</section>



					<section>
						<h2>
							Credit Scoring
						</h2>

						<ul>
							<li>Clasificación de clientes buenos y malos</li>
							<li>Modelos estadísticos sometidos a Basilea III</li>

							<li>Variables típicamente importantes como la cantidad pedida, número de pagos, tipo de interés, propiedades, ingresos anuales, etc.</li>
						</ul>
					</section>

				</section>

				<section>
					<section>
						<h2>
							Regulación
						</h2>
						<p>
							Basilea III y sus transposiciones:
						</p>
						<ul>
							<li>Directiva 2013/36/UE (CRD IV)</li>
							<li>Reglamento(UE) nº 575/2013</li>

						</ul>
						
					</section>
					<section>
						<h2>
							Reglamento 575/2013
						</h2>
						Artículo 169
						<blockquote>
							&ldquo;Cuando una entidad utilice diversos sistemas de calificación, deberá <span class="fragment highlight-blue">documentar</span> los criterios de asignación de cada deudor u <span class="fragment highlight-blue">operación a un sistema de calificación</span> determinado y aplicar dichos criterios de forma que reflejen adecuadamente el nivel de riesgo.&rdquo;
						</blockquote>
					</section>

					<section>
						<h2>
							Reglamento 575/2013
						</h2>
						Artículo 171
						<blockquote>
							&ldquo;La documentación del proceso de calificación <span class="fragment highlight-blue">permitirá a terceros entender cómo se asignan</span> las exposiciones a grados o a conjuntos de exposiciones, reproducir el proceso de asignación y evaluar la idoneidad de las asignaciones a un determinado grado o conjunto de exposiciones.&rdquo;
						</blockquote>
					</section>

					<section>
						<h2>
							Reglamento 575/2013
						</h2>
						Artículo 175
						<blockquote>
							&ldquo;La entidad dejará constancia escrita de los argumentos a favor y la <span class="fragment highlight-blue">lógica de los criterios de calificación elegidos</span>. [...] También deberá documentarse cómo se organiza la asignación de calificaciones, incluido el proceso de asignación de las calificaciones y la estructura de control interno.&rdquo;
						</blockquote>
					</section>
				</section>
			
				<section>
					<section>
						<h2>
							WOE/IV
						</h2>
						<ul>
							<li>Técnica para selección y reducción de variables</li>
							<li>El Weight of Evidence (WOE) mide el poder predictivo de una variable con respecto al target</li>
							<li>El Information Value (IV) mide la fuerza de esa relación</li>
						</ul>

					</section>
					<section>
						<h2>
							WOE/IV
						</h2>
						<ul>
							<img src="lib/woe_iv.png" alt="Down arrow" style="background: none;margin:none;border:none;box-shadow:none">
							
						</ul>

					</section>

				</section>

				<section>
					<section>
						<h2>
							Métodos de Machine Learning
						</h2>
						<ul>
							<li>Regresión logística</li>
							<li>Árboles de decisión</li>
							<li>Random Forest</li>
							<li>Extreme Gradient Boosting</li>
		
						</ul>
					</section>

				</section>
				<section>
					<section>
						<h2>
							Modelos de interpretabilidad
						</h2>
						<ul>
							<li>Eli5 (Explain me Like I'm 5)</li>
							<li>Lime (Local Interpretable Model-agnostic Explanations)</li>
							<li>Shap (SHapley Additive exPlanations)</li>
		
						</ul>
					</section>

					<section>
						<h2>
							Eli5
						</h2>
						<ul>
						<li>
							Creado para explicar modelos de caja blanca
						</li>
						<li>
							En los últimos tiempo ha implementado algoritmos para poder explicar modelos de caja negra, especialmente para sklearn
						</li>
						</ul>
					</section>

					<section>
						<h2>
							Lime
						</h2>
						<ul>
						<li>
							Explicaciones locales con perturbaciones en la observación buscada
						</li>
						</ul>
					</section>

					<section>
						<h2>
							Shap
						</h2>
						<ul>
						<li>
							Cálculo del Shap Value
						</li>
						<li>
							Muy complejo de calcular. Mucho tiempo de computación, por lo que se usa un subconjunto random del dataset
						</li>
						</ul>
					</section>
				</section>

				<section>
					<section>
						<h2>
							Experimentos
						</h2>
					</section>
					
					<section data-background="lib/back_lending.png" data-background-interactive>
						<div style="position: absolute; width: 40%; right: 0; box-shadow: 0 1px 4px rgba(0,0,0,0.5), 0 5px 25px rgba(0,0,0,0.2); background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px; font-size: 20px; text-align: left;">
							<h2>LendingClub</h2>
							<p>Servicio de préstamos entre particulares. Préstamos entre 1.000 y 35.000 dólares, y tasas entre 6,03% al 24,89%.</p>
						</div>
					</section>


					<section>
						<h2>
							Dataset
						</h2>
						<ul>
							<li>
								887.379 observaciones y 74 variables
							</li>
							<li>
								Partición en train y test manteniendo las proporción de morosos en el target: 621.165 en train y 266.214 en test
							</li>
						</ul>
					</section>

					<section>
						<h2>
							Preprocesado
						</h2>
						<ul>
							<li>
								Eliminamos las variables con más de un 10% de NAs
							</li>
							<li>
								Eliminamos las columnas con una correlación superior al 90%
							</li>
							<li>
								Mantenemos las variables categóricas tal y cómo están. Al hacer los pipelines, usaremos un OneHotEncoder. Esto es así porque lo necesita Lime.
							</li>
						</ul>
					</section>

					<section>
						<h2>
							Preprocesado
						</h2>
						<ul>
							<li>
								Nos quedamos con 33 columnas y un train con 577.834 observaciones y un test con 247.647. Esto es porque hay muchas variables con muchas nulos.
							</li>
							<li>
								Creamos una clase basada en el BaseEstimator de sklearn
							</li>

						</ul>
					</section>


					<section>
						<h2>
							Preprocesado
						</h2>
						<ul>

							<li>
								Además, es necesario crear un OrdinalEncoder en el fit, solo para crear luego un diccionario con la localización de cada variable y número de la categoría.
							</li>
							<li>
								Nuestro target pasa a ser la variable 'loan_status', dejando solo dos categorías: 'default' si ha presentado algún problema de pago, y 'fully paid' en caso contrario
							</li>
						</ul>
					</section>

					<section>
						<h2>
							Information Value
						</h2>
						<table>
							<thead>
								<tr>
									<th>Variable</th>
									<th>Information Value</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>issue_d</td>
									<td>0.780</td>
								</tr>
								<tr>
									<td>out_prnp</td>
									<td>0.705</td>
								</tr>
								<tr>
									<td>int_rate</td>
									<td>0.471</td>
								</tr>
								<tr>
									<td>sub_grade</td>
									<td>0.365</td>
								</tr>
								<tr>
									<td>total_rec_late_fee</td>
									<td>0.229</td>
								</tr>
							</tbody>
						</table>
					</section>

					<section>
						<h2>
							Accuracy
						</h2>
						<table>
							<thead>
								<tr>
									<th>Modelo</th>
									<th>Accuracy</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Regresión logística</td>
									<td>0.934</td>
								</tr>
								<tr>
									<td>Árboles de decisión</td>
									<td>0.925</td>
								</tr>
								<tr>
									<td>Random Forest</td>
									<td>0.971</td>
								</tr>
								<tr>
									<td>XGBoost</td>
									<td>0.987</td>
								</tr>
							</tbody>
						</table>
					</section>

					<section>
						<ul>
							<li>
								Pasamos a ver, para una observación determinada del test, las variables más importantes según cada modelo
							</li>
							<li>
								Hemos de recordar que son explicaciones locales, por lo que las variables importantes en otras observaciones pueden ser otras
							</li>
							<li>
								Aún así, si todas las explicaciones convergen, podemos estar más seguro de estar interpretando bien la clasificación
							</li>
						</ul>
					</section>
					<section>
						<h2>
							Regresión Logística 
						</h2>
						<h4>
							Variables más importantes
						</h4>
						<table>
							<thead>
								<tr>
									<th>Eli5</th>
									<th>Lime</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>total_pymnt</td>
									<td>total_pymnt</td>
								</tr>
								<tr>
									<td>total_rec_prncp</td>
									<td>total_rec_prncp</td>
								</tr>
								<tr>
									<td>total_rec_int</td>
									<td>recoveries</td>
								</tr>
								<tr>
									<td>loan_amnt</td>
									<td>total_rec_int</td>
								</tr>
								<tr>
									<td>out_prncp</td>
									<td>loan_amnt</td>
								</tr>
							</tbody>
						</table>
					</section>



<section>
						<h2>
							Árboles de decisión
						</h2>
						<h4>
							Variables más importantes
						</h4>
						<table>
							<thead>
								<tr>
									<th>Eli5</th>
									<th>Lime</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>last_pymnt_d</td>
									<td>last_pymnt_d</td>
								</tr>
								<tr>
									<td>last_pymnt_amnt</td>
									<td>total_rec_late_fee</td>
								</tr>
								<tr>
									<td>total_rec_prncp</td>
									<td>last_pymnt_amnt</td>
								</tr>
								<tr>
									<td></td>
									<td>acc_now_delinq</td>
								</tr>
								<tr>
									<td></td>
									<td>total_rec_prncp</td>
								</tr>
							</tbody>
						</table>
					</section>


<section>
						<h2>
							Random Forest
						</h2>
						<h4>
							Variables más importantes
						</h4>
						<table>
							<thead>
								<tr>
									<th>Eli5</th>
									<th>Lime</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>total_rec_prncp</td>
									<td>last_pymnt_d</td>
								</tr>
								<tr>
									<td>last_credit_pull_d</td>
									<td>recoveries</td>
								</tr>
								<tr>
									<td>recoveries</td>
									<td>collection_recovery_fee</td>
								</tr>
								<tr>
									<td>total_pymnt</td>
									<td>last_pymnt_amnt</td>
								</tr>
								<tr>
									<td>loan_amnt</td>
									<td>total_rec_prncp</td>
								</tr>
							</tbody>
						</table>
					</section>

					<section>
						<h2>
							Extreme Gradient Boosting 
						</h2>
						<h4>
							Variables más importantes
						</h4>
						<table>
							<thead>
								<tr>
									<th>Eli5</th>
									<th>Lime</th>
									<th>Shap</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>last_credit_pull_d</td>
									<td>recoveries</td>
									<td>out_prncp</td>
								</tr>
								<tr>
									<td>total_rec_prncp</td>
									<td>last_pymnt_d</td>
									<td>issue_d</td>
								</tr>
								<tr>
									<td>out_prncp</td>
									<td>out_prncp</td>
									<td>last_pymnt_amnt</td>
								</tr>
								<tr>
									<td>loan_amnt</td>
									<td>total_rec_prncp</td>
									<td>last_pymnt_d</td>
								</tr>
								<tr>
									<td>inq_last_6mths</td>
									<td>last_pymnt_amnt</td>
									<td>total_rec_prncp</td>
								</tr>
							</tbody>
						</table>
					</section>

					<section>
						<h2>
							Shap con XGBoost
						</h2>
						<ul>
							<img src="lib/shap_value.png" alt="Down arrow" style="background: none;margin:none;border:none;box-shadow:none">
							
						</ul>
					</section>

				</section>
				

				<section>
					<section>
						<h2>
							Conclusiones
						</h2>
						<ul>
						<li>
							Se puede explicar la clasificación de una observación con un método de ML. Saldrán modelos más certeros.
						</li>
						<li>
							Probablemente no sea suficiente para el regulador
						</li>
						<li>
							Modelos de interpretabilidad como Shap permiten tener una imagen general de cómo funciona la clasificación, pero son muy complejos computacionalmente
						</li>
						</ul>
					</section>


				</section>

				<section>
					<section>
						<h2>
							Futuros trabajos
						</h2>
						<ul>
						<li>Redes neuronales con Lime para datos tabulares</li>
						<li>Usar el discriminador de una GAN para clasificación</li>
						</ul>
					</section>


				</section>
			

				<section>
					<h3>
						Gracias por la atención
					</h3>
					<small>
						<a href="https://github.com/gustavovargas/tfm_afi">
						github.com/gustavovargas/tfm_afi</a>
					</small>
					<h4>
						Gustavo Vargas
					</h4>
					

				</section>



			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
